{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Q1: What is the primary goal of Natural Language Processing (NLP)?**  \n",
        "The primary goal of NLP is to enable computers to understand, interpret, and generate human language. This includes tasks like language understanding, sentiment analysis, translation, summarization, and question answering, with the ultimate aim of facilitating effective human-computer interaction using natural language.\n",
        "\n",
        "**Q2: What does \"tokenization\" refer to in text processing?**  \n",
        "Tokenization is the process of splitting text into smaller units called tokens. These tokens can be words, phrases, or sentences, depending on the granularity required. It's one of the first steps in text preprocessing, which makes the text easier to analyze.\n",
        "\n",
        "**Q3: What is the difference between lemmatization and stemming?**  \n",
        "Lemmatization and stemming are both techniques to reduce words to their base forms, but they work differently.  \n",
        "- **Stemming** cuts off prefixes or suffixes from words to reduce them to a root form, which might not always be a valid word (e.g., \"running\" becomes \"run\").  \n",
        "- **Lemmatization** takes a more sophisticated approach by considering the word's meaning and its part of speech, converting it to its proper base form (e.g., \"better\" becomes \"good\" and \"running\" becomes \"run\").\n",
        "\n",
        "**Q4: What is the role of regular expressions (regex) in text processing?**  \n",
        "Regular expressions (regex) are used to identify patterns in text. They allow for tasks such as searching for specific words or patterns (e.g., phone numbers, email addresses), replacing text, or extracting meaningful pieces of information from a text based on defined rules.\n",
        "\n",
        "**Q5: What is Word2Vec and how does it represent words in a vector space?**  \n",
        "Word2Vec is a model that learns vector representations (embeddings) of words by analyzing the context in which they appear in a large corpus of text. Each word is represented as a dense vector in a high-dimensional space, where words with similar meanings are placed near each other. This representation captures semantic relationships between words based on their co-occurrence patterns.\n",
        "\n",
        "**Q6: How does frequency distribution help in text analysis?**  \n",
        "Frequency distribution provides insights into the most common words or patterns in a text or corpus by counting the occurrence of each word. This helps in identifying important keywords, themes, or features that can be useful for text classification, topic modeling, and other NLP tasks.\n",
        "\n",
        "**Q7: Why is text normalization important in NLP?**  \n",
        "Text normalization is important because it standardizes the text and reduces variability, making it easier to process and analyze. This includes actions like converting all text to lowercase, removing punctuation, expanding contractions, and correcting spelling mistakes. It helps ensure that different forms of the same word are treated the same way and reduces noise in the data.\n",
        "\n",
        "\n",
        "**Q8: What is the difference between sentence tokenization and word tokenization?**  \n",
        "- **Sentence tokenization** splits text into individual sentences. It focuses on identifying sentence boundaries, typically using punctuation marks like periods, exclamation points, or question marks.\n",
        "- **Word tokenization** breaks the text into individual words. It focuses on identifying word boundaries, separating words by spaces, punctuation, or other language-specific rules.\n",
        "\n",
        "**Q9: What are co-occurrence vectors in NLP?**  \n",
        "Co-occurrence vectors represent the frequency with which words appear together in a certain context, like within a defined window of text. These vectors capture the relationships between words based on their co-occurrence patterns, which can be used to learn word embeddings or understand word associations in the text.\n",
        "\n",
        "**Q10: What is the significance of lemmatization in improving NLP tasks?**  \n",
        "Lemmatization improves NLP tasks by reducing words to their base or root form, which helps standardize word variants. This is especially useful for tasks like text classification, information retrieval, and machine translation, where understanding the core meaning of words (like \"better\" becoming \"good\") is crucial for accurate processing.\n",
        "\n",
        "**Q11: What is the primary use of word embeddings in NLP?**  \n",
        "The primary use of word embeddings is to represent words in a continuous, dense vector space where words with similar meanings or contexts are placed closer together. This helps capture semantic relationships, making them useful for tasks like text classification, machine translation, and sentiment analysis by allowing the model to understand word similarities and relationships more effectively.\n",
        "\n",
        "**Q12: What is an annotator in NLP?**  \n",
        "An annotator in NLP refers to a tool or person that adds annotations (labels or tags) to text data, providing information like part-of-speech tags, named entities, sentiment labels, or syntactic structures. These annotations are used for training machine learning models or improving the understanding of text data in various NLP tasks.\n",
        "\n",
        "**Q13: What are the key steps in text processing before applying machine learning models?**  \n",
        "Key steps in text processing typically include:\n",
        "1. **Tokenization**: Splitting text into tokens (words or sentences).\n",
        "2. **Text normalization**: Lowercasing, removing punctuation, correcting spelling.\n",
        "3. **Stop word removal**: Eliminating common but uninformative words.\n",
        "4. **Stemming or lemmatization**: Reducing words to their base forms.\n",
        "5. **Feature extraction**: Converting text into numerical representations, such as TF-IDF, bag-of-words, or word embeddings.\n",
        "\n",
        "**Q14: What is the history of NLP and how has it evolved?**  \n",
        "NLP's history began in the 1950s with rule-based systems and symbolic approaches. Early work focused on machine translation and syntactic analysis. In the 1980s and 1990s, statistical methods like hidden Markov models (HMMs) gained popularity. By the 2010s, the field experienced a revolution with the rise of deep learning, enabling significant advancements in models like Word2Vec, BERT, and GPT, which use neural networks and vast amounts of data for more accurate and context-aware language understanding.\n",
        "\n",
        "**Q15: Why is sentence processing important in NLP?**  \n",
        "Sentence processing is crucial because it helps machines understand sentence structure, grammar, and meaning. By analyzing sentence boundaries and syntax, NLP models can parse the relationships between words and understand the overall context of a sentence. This is essential for tasks such as machine translation, text summarization, and question answering.\n",
        "\n",
        "\n",
        "**Q16: How do word embeddings improve the understanding of language semantics in NLP?**  \n",
        "Word embeddings represent words as dense vectors in a continuous vector space, where words with similar meanings are placed closer together. By capturing semantic relationships based on context, word embeddings allow NLP models to understand the meaning of words in relation to each other, enhancing tasks like sentiment analysis, text classification, and machine translation.\n",
        "\n",
        "**Q17: How does the frequency distribution of words help in text classification?**  \n",
        "Frequency distribution helps in text classification by identifying which words are most important or prevalent in a given text or corpus. It allows for the extraction of features like the term frequency (TF) or term frequency-inverse document frequency (TF-IDF), which are used to classify texts based on the occurrence patterns of specific words. More frequent words often provide valuable information for categorizing content.\n",
        "\n",
        "**Q18: What are the advantages of using regex in text cleaning?**  \n",
        "Regex (regular expressions) allows for efficient pattern matching, which can help in cleaning text by:\n",
        "- Extracting specific patterns like dates, phone numbers, or emails.\n",
        "- Replacing or removing unwanted characters, such as punctuation or special symbols.\n",
        "- Standardizing text (e.g., converting to lowercase, removing extra spaces).\n",
        "Regex provides flexibility and power for custom cleaning tasks.\n",
        "\n",
        "**Q19: What is the difference between Word2Vec and Doc2Vec?**  \n",
        "- **Word2Vec** generates vector representations for individual words by analyzing their context in a corpus. The resulting vectors capture semantic relationships between words.\n",
        "- **Doc2Vec** extends the idea of Word2Vec to entire documents (or sentences). It generates vector representations for longer chunks of text, enabling models to capture the semantic meaning of a document as a whole, rather than just individual words.\n",
        "\n",
        "**Q20: Why is understanding text normalization important in NLP?**  \n",
        "Text normalization is crucial because it standardizes raw text to reduce inconsistencies, making it easier for NLP models to process. By converting text to a uniform format (e.g., lowercase, removing punctuation), models can focus on meaningful patterns without being distracted by superficial variations like case differences or extra characters. This is essential for accurate text analysis and improving the performance of downstream tasks like classification or translation.\n",
        "\n",
        "**Q21: How does word count help in text analysis?**  \n",
        "Word count helps in text analysis by providing insights into the text's length, structure, and the relative importance of certain words. It can be used to identify key terms (by looking at frequency counts) or to analyze document length for tasks like text summarization, document similarity, and feature extraction in machine learning models.\n",
        "\n",
        "**Q22: How does lemmatization help in NLP tasks like search engines and chatbots?**  \n",
        "Lemmatization helps search engines and chatbots by reducing words to their base forms, ensuring that different variations of a word (e.g., \"running\" and \"ran\") are treated as the same word. This improves search accuracy by allowing users to retrieve relevant results regardless of the word form they use. In chatbots, lemmatization helps match user queries with predefined responses by recognizing variations in word forms.\n",
        "\n",
        "**Q23: What is the purpose of using Doc2Vec in text processing?**  \n",
        "Doc2Vec is used to generate vector representations for entire documents or sentences, capturing their overall meaning. This is helpful in tasks like document classification, clustering, and similarity analysis, where understanding the context and semantics of longer text sequences is important, rather than just individual words.\n",
        "\n",
        "**Q24: What is the importance of sentence processing in NLP?**  \n",
        "Sentence processing is important because it helps break down the structure and meaning of a sentence, including the relationships between words. Proper sentence processing is essential for tasks such as machine translation, sentiment analysis, and question answering, where understanding the sentence's grammatical structure and contextual meaning is key to providing accurate results.\n",
        "\n",
        "**Q25: What is text normalization, and what are the common techniques used in it?**  \n",
        "Text normalization is the process of transforming text into a standardized format to reduce variation and noise. Common techniques include:\n",
        "- **Lowercasing**: Converting all text to lowercase to treat case variations consistently.\n",
        "- **Removing punctuation**: Eliminating punctuation marks that donâ€™t contribute to meaning.\n",
        "- **Expanding contractions**: Converting contractions like \"don't\" to \"do not\" for consistency.\n",
        "- **Spell correction**: Fixing typos and spelling errors.\n",
        "- **Removing stop words**: Eliminating common but uninformative words (e.g., \"and,\" \"the\").\n",
        "These techniques help ensure that the text is clean and uniform for further analysis.\n",
        "\n",
        "**Q26: Why is word tokenization important in NLP?**  \n",
        "Word tokenization is important because it breaks text into smaller, manageable units (words) that can be analyzed and processed. By splitting the text into words, NLP models can better understand and extract meaning from the text, whether for tasks like sentiment analysis, information retrieval, or machine translation.\n",
        "\n",
        "**Q27: How does sentence tokenization differ from word tokenization in NLP?**  \n",
        "- **Sentence tokenization** divides text into individual sentences, using punctuation marks (like periods or exclamation points) to identify sentence boundaries.\n",
        "- **Word tokenization** divides text into individual words, focusing on word boundaries such as spaces or punctuation marks.\n",
        "Sentence tokenization is about structuring the text at the sentence level, while word tokenization focuses on individual words.\n",
        "\n",
        "**Q28: What is the primary purpose of text processing in NLP?**  \n",
        "The primary purpose of text processing is to prepare raw text for further analysis and model training by cleaning, organizing, and standardizing the data. This includes tasks like tokenization, removing noise (e.g., stop words), and transforming text into a format suitable for machine learning models, making it easier for algorithms to extract useful insights.\n",
        "\n",
        "**Q29: What are the key challenges in NLP?**  \n",
        "Some key challenges in NLP include:\n",
        "- **Ambiguity**: Words or phrases can have multiple meanings depending on context.\n",
        "- **Language variability**: The same meaning can be expressed in many different ways.\n",
        "- **Context understanding**: Understanding the context, including sarcasm or implied meaning, can be difficult for machines.\n",
        "- **Data sparsity**: Limited labeled data for training models can hinder performance.\n",
        "- **Handling different languages and dialects**: Models need to work across multiple languages and adapt to diverse syntaxes and vocabulary.\n",
        "\n",
        "**Q30: How do co-occurrence vectors represent relationships between words?**  \n",
        "Co-occurrence vectors represent the relationships between words by counting how often pairs of words appear together within a certain context window. The vector captures how strongly associated two words are by their frequency of co-occurrence. For example, words like \"king\" and \"queen\" will have similar co-occurrence vectors because they often appear in similar contexts.\n",
        "\n",
        "**Q31: What is the role of frequency distribution in text analysis?**  \n",
        "Frequency distribution helps in text analysis by identifying the most common or important words in a corpus. This can provide valuable insights into the text's themes and structure, guide feature extraction for machine learning, and be used to filter out irrelevant words (e.g., stop words) for tasks like text classification and clustering.\n",
        "\n",
        "**Q32: What is the impact of word embeddings on NLP tasks?**  \n",
        "Word embeddings improve NLP tasks by representing words in a dense vector space where semantically similar words are placed near each other. This enables models to understand nuances like word similarity, context, and relationships, which is beneficial for tasks like sentiment analysis, machine translation, and question answering.\n",
        "\n",
        "**Q33= What is the purpose of using lemmatization in text preprocessing?**  \n",
        "The purpose of using lemmatization in text preprocessing is to reduce words to their base or root form (lemma) based on their meaning and part of speech. This standardizes words with similar meanings, improving the model's ability to process them consistently, which is especially helpful in tasks like search engines, chatbots, and machine translation where understanding word variations is key to accuracy."
      ],
      "metadata": {
        "id": "0tuGLbouQxjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Practical"
      ],
      "metadata": {
        "id": "IiWmRHa_ShtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q1.How can you perform word tokenization using NLTK?"
      ],
      "metadata": {
        "id": "U9WpFeqVSj8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Download necessary data\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"This is an example sentence.\"\n",
        "words = word_tokenize(text)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "sQ2K4YHLTIm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q2. How can you perform sentence tokenization using NLTK?"
      ],
      "metadata": {
        "id": "OQKCb0T0SwID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"This is the first sentence. This is the second sentence.\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "GsmdumGuS0Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q3. How can you remove stopwords from a sentence?"
      ],
      "metadata": {
        "id": "Oh7ZG2TES2oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "text = \"This is an example sentence with some stopwords.\"\n",
        "words = word_tokenize(text)\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "id": "8-Fnvf3-TLQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q4.  How can you perform stemming on a word?"
      ],
      "metadata": {
        "id": "K9Z7UkraTOq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word = \"running\"\n",
        "lemmatized_word = lemmatizer.lemmatize(word, pos='v')  # 'v' for verb\n",
        "print(lemmatized_word)\n"
      ],
      "metadata": {
        "id": "3Gy5UlisTUud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q5.  How can you perform lemmatization on a word?"
      ],
      "metadata": {
        "id": "CnHmqm-VTW-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word = \"running\"\n",
        "lemmatized_word = lemmatizer.lemmatize(word, pos='v')  # 'v' for verb\n",
        "print(lemmatized_word)\n"
      ],
      "metadata": {
        "id": "qJ5CdzYlTdP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q6. How can you normalize a text by converting it to lowercase and removing punctuation?"
      ],
      "metadata": {
        "id": "uENNlc3xTdMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"This is an example sentence!\"\n",
        "normalized_text = text.lower()  # Convert to lowercase\n",
        "normalized_text = normalized_text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "print(normalized_text)\n"
      ],
      "metadata": {
        "id": "YoX4_ai8TdJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q7. How can you create a co-occurrence matrix for words in a corpus?"
      ],
      "metadata": {
        "id": "MTIQMJi3TdGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"this is a sample text\",\n",
        "    \"this is another example text\",\n",
        "    \"sample text example\"\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "co_occurrence_matrix = (X.T * X).toarray()\n",
        "\n",
        "print(co_occurrence_matrix)\n"
      ],
      "metadata": {
        "id": "0gJCGULaTdDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q8. How can you apply a regular expression to extract all email addresses from a text?"
      ],
      "metadata": {
        "id": "6KiUXOBETc_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Contact us at support@example.com or info@example.org.\"\n",
        "emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', text)\n",
        "print(emails)\n"
      ],
      "metadata": {
        "id": "p0HQqWqiTc7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q9. How can you perform word embedding using Word2Vec?"
      ],
      "metadata": {
        "id": "rIR08-tPTxM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Example corpus\n",
        "sentences = [\n",
        "    ['this', 'is', 'an', 'example'],\n",
        "    ['this', 'is', 'another', 'sentence']\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "word_vector = model.wv['example']  # Get the vector for 'example'\n",
        "print(word_vector)\n"
      ],
      "metadata": {
        "id": "gsjk-DkQTxKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q10. How can you use Doc2Vec to embed documents?"
      ],
      "metadata": {
        "id": "faC5jCegTxHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# Example corpus with documents tagged\n",
        "documents = [\n",
        "    TaggedDocument(words=['this', 'is', 'document', 'one'], tags=['doc1']),\n",
        "    TaggedDocument(words=['this', 'is', 'document', 'two'], tags=['doc2'])\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "model = Doc2Vec(documents, vector_size=20, window=2, min_count=1, workers=4)\n",
        "doc_vector = model.dv['doc1']  # Get the vector for 'doc1'\n",
        "print(doc_vector)\n"
      ],
      "metadata": {
        "id": "u3P4AB-gTxAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q11.How can you perform part-of-speech tagging?"
      ],
      "metadata": {
        "id": "F32x9o5DUJj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "text = \"This is a simple sentence.\"\n",
        "words = word_tokenize(text)\n",
        "tagged_words = pos_tag(words)\n",
        "print(tagged_words)\n"
      ],
      "metadata": {
        "id": "RgOXQZw6UJf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q12. How can you find the similarity between two sentences using cosine similarity?"
      ],
      "metadata": {
        "id": "GFb-hRPuUJbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "sentences = [\"This is a sample sentence.\", \"This is another sentence.\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "print(similarity_matrix)\n"
      ],
      "metadata": {
        "id": "FEFKOzV-Tw7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q13: How can you extract named entities from a sentence?"
      ],
      "metadata": {
        "id": "tOGYFat5UV5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "sentence = \"Barack Obama was born in Hawaii.\"\n",
        "words = word_tokenize(sentence)\n",
        "tags = pos_tag(words)\n",
        "tree = ne_chunk(tags)\n",
        "\n",
        "# Print the named entities\n",
        "print(tree)\n"
      ],
      "metadata": {
        "id": "qul60LddUV2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q14: How can you split a large document into smaller chunks of text?"
      ],
      "metadata": {
        "id": "CPD5uja6UVx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"This is the first paragraph. This is the second paragraph. Here's the third paragraph.\"\n",
        "chunks = sent_tokenize(text)\n",
        "\n",
        "# Print the resulting chunks\n",
        "for chunk in chunks:\n",
        "    print(chunk)\n"
      ],
      "metadata": {
        "id": "XC4hcIFuUVqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q15: How can you calculate the TF-IDF (Term Frequency - Inverse Document Frequency) for a set of documents?"
      ],
      "metadata": {
        "id": "YW31zb8vTc2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents = [\n",
        "    \"This is a sample document.\",\n",
        "    \"This document is another example.\",\n",
        "    \"Text processing is fun and interesting.\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert to array for viewing\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "print(tfidf_array)\n",
        "\n",
        "# Display feature names (words)\n",
        "print(vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "id": "3IKvcnImVLj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q16: How can you apply tokenization, stopword removal, and stemming in one go?"
      ],
      "metadata": {
        "id": "qor6otMGVNzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "text = \"This is a simple example for tokenization, stopword removal, and stemming.\"\n",
        "\n",
        "# Tokenize\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords and apply stemming\n",
        "processed_tokens = [stemmer.stem(word) for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(processed_tokens)\n"
      ],
      "metadata": {
        "id": "shugi25bVi67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q17: How can you visualize the frequency distribution of words in a sentence?"
      ],
      "metadata": {
        "id": "8NWsR9PPVk_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentence = \"This is a simple example sentence. This is another example.\"\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "# Calculate frequency distribution\n",
        "fdist = nltk.FreqDist(words)\n",
        "\n",
        "# Plot the frequency distribution\n",
        "fdist.plot(title=\"Word Frequency Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6PznLKoLVokV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}